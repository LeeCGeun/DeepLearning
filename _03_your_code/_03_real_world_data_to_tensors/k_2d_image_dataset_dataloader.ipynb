{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogCat2DImageDataset(Dataset):\n",
    "  def __init__(self):\n",
    "    # preprecessing pipeline\n",
    "    self.image_transforms = transforms.Compose([\n",
    "      transforms.Resize(size=(256, 256)),\n",
    "      transforms.ToTensor() # ToTensor : HxWxC -> CxHxW & 0~255 -> 0~1 Normalizing\n",
    "    ])\n",
    "\n",
    "    dogs_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"a_image-dog\")\n",
    "    cats_dir = os.path.join(os.path.pardir, os.path.pardir, \"_00_data\", \"b_image-cats\")\n",
    "\n",
    "    image_lst = [\n",
    "      Image.open(os.path.join(dogs_dir, \"bobby.jpg\")),  # (1280, 720, 3)\n",
    "      Image.open(os.path.join(cats_dir, \"cat1.png\")),  # (256, 256, 3)\n",
    "      Image.open(os.path.join(cats_dir, \"cat2.png\")),  # (256, 256, 3)\n",
    "      Image.open(os.path.join(cats_dir, \"cat3.png\"))  # (256, 256, 3)\n",
    "    ]\n",
    "\n",
    "    image_lst = [self.image_transforms(img) for img in image_lst] # [3, 256, 256]\n",
    "    self.images = torch.stack(image_lst, dim=0) # [4, 3, 256, 256]\n",
    "\n",
    "    # 0: \"dog\", 1: \"cat\"\n",
    "    self.image_labels = torch.tensor([[0], [1], [1], [1]]) #[4,1]\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.images[idx], self.image_labels[idx]\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.images), self.images.shape, self.image_labels.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 4, Input Shape: torch.Size([4, 3, 256, 256]), Target Shape: torch.Size([4, 1])\n",
      "################################################## 1\n",
      "0 - torch.Size([3, 256, 256]): tensor([0])\n",
      "1 - torch.Size([3, 256, 256]): tensor([1])\n",
      "2 - torch.Size([3, 256, 256]): tensor([1])\n",
      "3 - torch.Size([3, 256, 256]): tensor([1])\n",
      "################################################## 2\n",
      "3 1\n",
      "################################################## 3\n",
      "0 - torch.Size([2, 3, 256, 256]): tensor([[0],\n",
      "        [1]])\n",
      "1 - torch.Size([1, 3, 256, 256]): tensor([[1]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.6118, 0.5961, 0.4863,  ..., 0.5882, 0.5843, 0.6196],\n",
       "          [0.6824, 0.5255, 0.6471,  ..., 0.4706, 0.5333, 0.5412],\n",
       "          [0.4980, 0.6118, 0.4196,  ..., 0.5137, 0.5608, 0.6431],\n",
       "          ...,\n",
       "          [0.4549, 0.5098, 0.5059,  ..., 0.4980, 0.4627, 0.4392],\n",
       "          [0.5059, 0.5098, 0.4824,  ..., 0.4510, 0.4745, 0.4471],\n",
       "          [0.5059, 0.4824, 0.4627,  ..., 0.4431, 0.4745, 0.4706]],\n",
       "\n",
       "         [[0.5451, 0.5294, 0.4275,  ..., 0.5294, 0.5294, 0.5765],\n",
       "          [0.6275, 0.4667, 0.5843,  ..., 0.4118, 0.4784, 0.4863],\n",
       "          [0.4431, 0.5490, 0.3529,  ..., 0.4627, 0.5059, 0.5961],\n",
       "          ...,\n",
       "          [0.3882, 0.4314, 0.4353,  ..., 0.4588, 0.4235, 0.4039],\n",
       "          [0.4353, 0.4353, 0.4157,  ..., 0.4157, 0.4392, 0.4118],\n",
       "          [0.4353, 0.4078, 0.4000,  ..., 0.4039, 0.4314, 0.4353]],\n",
       "\n",
       "         [[0.5059, 0.4824, 0.3843,  ..., 0.5137, 0.5176, 0.5686],\n",
       "          [0.6078, 0.4314, 0.5373,  ..., 0.4000, 0.4667, 0.4745],\n",
       "          [0.4078, 0.5176, 0.3137,  ..., 0.4392, 0.4902, 0.5725],\n",
       "          ...,\n",
       "          [0.3647, 0.4235, 0.4118,  ..., 0.4902, 0.4510, 0.4235],\n",
       "          [0.4235, 0.4235, 0.3843,  ..., 0.4314, 0.4588, 0.4314],\n",
       "          [0.4196, 0.3843, 0.3725,  ..., 0.4235, 0.4510, 0.4549]]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  dog_cat_2d_image_dataset = DogCat2DImageDataset()\n",
    "\n",
    "  print(dog_cat_2d_image_dataset)\n",
    "\n",
    "  print(\"#\" * 50, 1)\n",
    "\n",
    "  for idx, sample in enumerate(dog_cat_2d_image_dataset):\n",
    "    input, target = sample\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target))\n",
    "\n",
    "  train_dataset, test_dataset = random_split(dog_cat_2d_image_dataset, [0.7, 0.3])\n",
    "\n",
    "  print(\"#\" * 50, 2)\n",
    "\n",
    "  print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "  print(\"#\" * 50, 3)\n",
    "\n",
    "  train_data_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=True\n",
    "  )\n",
    "\n",
    "  for idx, batch in enumerate(train_data_loader):\n",
    "    input, target = batch\n",
    "    print(\"{0} - {1}: {2}\".format(idx, input.shape, target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "link_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
