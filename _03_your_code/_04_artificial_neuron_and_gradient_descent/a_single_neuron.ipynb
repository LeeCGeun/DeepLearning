{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import nbimporter\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거\n",
    "\n",
    "from a_single_neuron import model, loss_fn, SimpleDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\git_deeplearning\\\\DeepLearning\\\\_03_your_code\\\\_04_artificial_neuron_and_gradient_descent'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super().__init__(*args, **kwargs)\n",
    "\n",
    "    X = [[0.5, 0.9], [14.0, 12.0], [15.0, 13.6],\n",
    "         [28.0, 22.8], [11.0, 8.1], [8.0, 7.1],\n",
    "         [3.0, 2.9], [4.0, 0.1], [6.0, 5.3],\n",
    "         [13.0, 12.0], [21.0, 19.9], [-1.0, 1.5]]\n",
    "\n",
    "    y = [35.7, 55.9, 58.2, 81.9, 56.3, 48.9, 33.9, 21.8, 48.4, 60.4, 68.4, 29.1]\n",
    "\n",
    "    self.X = torch.tensor(X, dtype=torch.float, device=device)\n",
    "    self.y = torch.tensor(y, dtype=torch.float, device=device)\n",
    "    self.y = self.y * 0.01\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.X[idx], self.y[idx]\n",
    "\n",
    "  def __str__(self):\n",
    "    str = \"Data Size: {0}, Input Shape: {1}, Target Shape: {2}\".format(\n",
    "      len(self.X), self.X.shape, self.y.shape\n",
    "    )\n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, W, b):\n",
    "  # print(X.shape)  # >>> torch.Size([12, 2])\n",
    "  # print(W.shape)  # >>> torch.Size([2])\n",
    "  # print(b.shape)  # >>> torch.Size([1])\n",
    "  u = torch.sum(X * W, dim=1) + b\n",
    "  z = activate(u)\n",
    "  return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate(u):\n",
    "  return F.sigmoid(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(y_pred, y):\n",
    "  loss = torch.square(y_pred - y).mean()\n",
    "  assert loss.shape == () or loss.shape == (1,)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(W, b, X, y):\n",
    "  # W.shape: (2,), b.shape: (1,), X.shape: (12, 2), y.shape: (12)\n",
    "  y_pred = model(X, W, b)\n",
    "  dl_dy = 2 * (y_pred - y)\n",
    "  dl_dy = dl_dy.unsqueeze(dim=-1)  # dl_dy_pred.shape: [12, 1]\n",
    "\n",
    "  dy_df = 1.0\n",
    "\n",
    "  z = torch.sum(X * W, dim=-1) + b\n",
    "  ds_dz = activate(z) * (1.0 - activate(z))\n",
    "  ds_dz = ds_dz.unsqueeze(dim=-1)  # ds_dz_pred.shape: [12, 1]\n",
    "\n",
    "  W_grad = torch.mean(dl_dy * dy_df * ds_dz * X, dim=0)\n",
    "  b_grad = torch.mean(dl_dy * dy_df * ds_dz * 1.0, dim=0)\n",
    "\n",
    "  return W_grad, b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(W, b, train_data_loader):\n",
    "  MAX_EPOCHS = 20_000\n",
    "  LEARNING_RATE = 0.01\n",
    "\n",
    "  for epoch in range(0, MAX_EPOCHS):\n",
    "    batch = next(iter(train_data_loader))\n",
    "    input, target = batch\n",
    "    y_pred = model(input, W, b)\n",
    "    loss = loss_fn(y_pred, target)\n",
    "\n",
    "    W_grad, b_grad = gradient(W, b, input, target)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "      print(\"[Epoch:{0:6,}] loss:{1:8.5f}, w0:{2:6.3f}, w1:{3:6.3f}, b:{4:6.3f}\".format(\n",
    "        epoch, loss.item(), W[0].item(), W[1].item(), b.item()\n",
    "      ), end=\", \")\n",
    "      print(\"W.grad: {0}, b.grad:{1}\".format(W_grad, b_grad))\n",
    "\n",
    "    W = W - LEARNING_RATE * W_grad\n",
    "    b = b - LEARNING_RATE * b_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12])\n",
      "tensor([0.8022, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9973, 0.9837, 1.0000,\n",
      "        1.0000, 1.0000, 0.6225])\n",
      "tensor(0.2254)\n",
      "[Epoch:     0] loss: 0.22539, w0: 1.000, w1: 1.000, b: 0.000, W.grad: tensor([0.0020, 0.0311]), b.grad:tensor([0.0271])\n",
      "[Epoch:   100] loss: 0.22369, w0: 0.998, w1: 0.969, b:-0.027, W.grad: tensor([0.0030, 0.0307]), b.grad:tensor([0.0269])\n",
      "[Epoch:   200] loss: 0.22203, w0: 0.994, w1: 0.939, b:-0.054, W.grad: tensor([0.0040, 0.0302]), b.grad:tensor([0.0267])\n",
      "[Epoch:   300] loss: 0.22041, w0: 0.989, w1: 0.909, b:-0.080, W.grad: tensor([0.0052, 0.0297]), b.grad:tensor([0.0264])\n",
      "[Epoch:   400] loss: 0.21882, w0: 0.984, w1: 0.879, b:-0.107, W.grad: tensor([0.0064, 0.0291]), b.grad:tensor([0.0261])\n",
      "[Epoch:   500] loss: 0.21727, w0: 0.977, w1: 0.850, b:-0.133, W.grad: tensor([0.0077, 0.0285]), b.grad:tensor([0.0258])\n",
      "[Epoch:   600] loss: 0.21575, w0: 0.968, w1: 0.822, b:-0.158, W.grad: tensor([0.0092, 0.0279]), b.grad:tensor([0.0254])\n",
      "[Epoch:   700] loss: 0.21425, w0: 0.958, w1: 0.795, b:-0.183, W.grad: tensor([0.0107, 0.0273]), b.grad:tensor([0.0251])\n",
      "[Epoch:   800] loss: 0.21276, w0: 0.947, w1: 0.768, b:-0.208, W.grad: tensor([0.0124, 0.0268]), b.grad:tensor([0.0248])\n",
      "[Epoch:   900] loss: 0.21128, w0: 0.934, w1: 0.741, b:-0.233, W.grad: tensor([0.0142, 0.0263]), b.grad:tensor([0.0246])\n",
      "[Epoch: 1,000] loss: 0.20977, w0: 0.918, w1: 0.715, b:-0.258, W.grad: tensor([0.0162, 0.0258]), b.grad:tensor([0.0243])\n",
      "[Epoch: 1,100] loss: 0.20823, w0: 0.901, w1: 0.689, b:-0.282, W.grad: tensor([0.0184, 0.0255]), b.grad:tensor([0.0242])\n",
      "[Epoch: 1,200] loss: 0.20661, w0: 0.881, w1: 0.664, b:-0.306, W.grad: tensor([0.0210, 0.0253]), b.grad:tensor([0.0241])\n",
      "[Epoch: 1,300] loss: 0.20489, w0: 0.859, w1: 0.639, b:-0.330, W.grad: tensor([0.0239, 0.0252]), b.grad:tensor([0.0241])\n",
      "[Epoch: 1,400] loss: 0.20301, w0: 0.833, w1: 0.614, b:-0.354, W.grad: tensor([0.0274, 0.0253]), b.grad:tensor([0.0243])\n",
      "[Epoch: 1,500] loss: 0.20090, w0: 0.804, w1: 0.588, b:-0.379, W.grad: tensor([0.0315, 0.0257]), b.grad:tensor([0.0247])\n",
      "[Epoch: 1,600] loss: 0.19844, w0: 0.770, w1: 0.562, b:-0.404, W.grad: tensor([0.0367, 0.0265]), b.grad:tensor([0.0253])\n",
      "[Epoch: 1,700] loss: 0.19545, w0: 0.730, w1: 0.535, b:-0.429, W.grad: tensor([0.0432, 0.0278]), b.grad:tensor([0.0263])\n",
      "[Epoch: 1,800] loss: 0.19167, w0: 0.683, w1: 0.506, b:-0.456, W.grad: tensor([0.0516, 0.0298]), b.grad:tensor([0.0277])\n",
      "[Epoch: 1,900] loss: 0.18661, w0: 0.626, w1: 0.475, b:-0.485, W.grad: tensor([0.0628, 0.0331]), b.grad:tensor([0.0299])\n",
      "[Epoch: 2,000] loss: 0.17944, w0: 0.556, w1: 0.439, b:-0.516, W.grad: tensor([0.0780, 0.0386]), b.grad:tensor([0.0329])\n",
      "[Epoch: 2,100] loss: 0.16868, w0: 0.469, w1: 0.396, b:-0.551, W.grad: tensor([0.0982, 0.0482]), b.grad:tensor([0.0369])\n",
      "[Epoch: 2,200] loss: 0.15149, w0: 0.358, w1: 0.340, b:-0.590, W.grad: tensor([0.1261, 0.0680]), b.grad:tensor([0.0413])\n",
      "[Epoch: 2,300] loss: 0.11672, w0: 0.205, w1: 0.247, b:-0.634, W.grad: tensor([0.2003, 0.1402]), b.grad:tensor([0.0466])\n",
      "[Epoch: 2,400] loss: 0.00297, w0: 0.000, w1: 0.083, b:-0.667, W.grad: tensor([-0.0003, -0.0002]), b.grad:tensor([0.0069])\n",
      "[Epoch: 2,500] loss: 0.00293, w0: 0.001, w1: 0.083, b:-0.674, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0066])\n",
      "[Epoch: 2,600] loss: 0.00288, w0: 0.001, w1: 0.084, b:-0.681, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0064])\n",
      "[Epoch: 2,700] loss: 0.00284, w0: 0.001, w1: 0.084, b:-0.687, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0061])\n",
      "[Epoch: 2,800] loss: 0.00281, w0: 0.001, w1: 0.084, b:-0.693, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0059])\n",
      "[Epoch: 2,900] loss: 0.00278, w0: 0.001, w1: 0.084, b:-0.699, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0056])\n",
      "[Epoch: 3,000] loss: 0.00275, w0: 0.002, w1: 0.084, b:-0.704, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0054])\n",
      "[Epoch: 3,100] loss: 0.00272, w0: 0.002, w1: 0.085, b:-0.709, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0052])\n",
      "[Epoch: 3,200] loss: 0.00269, w0: 0.002, w1: 0.085, b:-0.714, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0050])\n",
      "[Epoch: 3,300] loss: 0.00267, w0: 0.002, w1: 0.085, b:-0.719, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0048])\n",
      "[Epoch: 3,400] loss: 0.00265, w0: 0.002, w1: 0.085, b:-0.724, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0046])\n",
      "[Epoch: 3,500] loss: 0.00263, w0: 0.003, w1: 0.085, b:-0.728, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0044])\n",
      "[Epoch: 3,600] loss: 0.00261, w0: 0.003, w1: 0.085, b:-0.733, W.grad: tensor([-0.0002, -0.0002]), b.grad:tensor([0.0042])\n",
      "[Epoch: 3,700] loss: 0.00259, w0: 0.003, w1: 0.086, b:-0.737, W.grad: tensor([-0.0001, -0.0001]), b.grad:tensor([0.0040])\n",
      "[Epoch: 3,800] loss: 0.00258, w0: 0.003, w1: 0.086, b:-0.741, W.grad: tensor([-0.0001, -0.0001]), b.grad:tensor([0.0039])\n",
      "[Epoch: 3,900] loss: 0.00256, w0: 0.003, w1: 0.086, b:-0.745, W.grad: tensor([-0.0001, -0.0001]), b.grad:tensor([0.0037])\n",
      "[Epoch: 4,000] loss: 0.00255, w0: 0.003, w1: 0.086, b:-0.748, W.grad: tensor([-0.0001, -0.0001]), b.grad:tensor([0.0036])\n",
      "[Epoch: 4,100] loss: 0.00254, w0: 0.003, w1: 0.086, b:-0.752, W.grad: tensor([-0.0001, -0.0001]), b.grad:tensor([0.0034])\n",
      "[Epoch: 4,200] loss: 0.00252, w0: 0.003, w1: 0.086, b:-0.755, W.grad: tensor([-0.0001, -0.0001]), b.grad:tensor([0.0033])\n",
      "[Epoch: 4,300] loss: 0.00251, w0: 0.004, w1: 0.086, b:-0.758, W.grad: tensor([-0.0001, -0.0001]), b.grad:tensor([0.0032])\n",
      "[Epoch: 4,400] loss: 0.00250, w0: 0.004, w1: 0.086, b:-0.761, W.grad: tensor([-0.0001, -0.0001]), b.grad:tensor([0.0030])\n",
      "[Epoch: 4,500] loss: 0.00250, w0: 0.004, w1: 0.087, b:-0.764, W.grad: tensor([-0.0001, -0.0001]), b.grad:tensor([0.0029])\n",
      "[Epoch: 4,600] loss: 0.00249, w0: 0.004, w1: 0.087, b:-0.767, W.grad: tensor([-1.0407e-04, -9.8312e-05]), b.grad:tensor([0.0028])\n",
      "[Epoch: 4,700] loss: 0.00248, w0: 0.004, w1: 0.087, b:-0.770, W.grad: tensor([-1.0029e-04, -9.4163e-05]), b.grad:tensor([0.0027])\n",
      "[Epoch: 4,800] loss: 0.00247, w0: 0.004, w1: 0.087, b:-0.773, W.grad: tensor([-9.6715e-05, -9.0243e-05]), b.grad:tensor([0.0026])\n",
      "[Epoch: 4,900] loss: 0.00247, w0: 0.004, w1: 0.087, b:-0.775, W.grad: tensor([-9.3237e-05, -8.6442e-05]), b.grad:tensor([0.0025])\n",
      "[Epoch: 5,000] loss: 0.00246, w0: 0.004, w1: 0.087, b:-0.778, W.grad: tensor([-8.9817e-05, -8.2749e-05]), b.grad:tensor([0.0024])\n",
      "[Epoch: 5,100] loss: 0.00245, w0: 0.004, w1: 0.087, b:-0.780, W.grad: tensor([-8.6608e-05, -7.9282e-05]), b.grad:tensor([0.0023])\n",
      "[Epoch: 5,200] loss: 0.00245, w0: 0.004, w1: 0.087, b:-0.782, W.grad: tensor([-8.3544e-05, -7.5991e-05]), b.grad:tensor([0.0022])\n",
      "[Epoch: 5,300] loss: 0.00244, w0: 0.005, w1: 0.087, b:-0.784, W.grad: tensor([-8.0445e-05, -7.2695e-05]), b.grad:tensor([0.0021])\n",
      "[Epoch: 5,400] loss: 0.00244, w0: 0.005, w1: 0.087, b:-0.786, W.grad: tensor([-7.7628e-05, -6.9694e-05]), b.grad:tensor([0.0020])\n",
      "[Epoch: 5,500] loss: 0.00244, w0: 0.005, w1: 0.087, b:-0.788, W.grad: tensor([-7.4733e-05, -6.6672e-05]), b.grad:tensor([0.0020])\n",
      "[Epoch: 5,600] loss: 0.00243, w0: 0.005, w1: 0.087, b:-0.790, W.grad: tensor([-7.2271e-05, -6.4051e-05]), b.grad:tensor([0.0019])\n",
      "[Epoch: 5,700] loss: 0.00243, w0: 0.005, w1: 0.088, b:-0.792, W.grad: tensor([-6.9728e-05, -6.1398e-05]), b.grad:tensor([0.0018])\n",
      "[Epoch: 5,800] loss: 0.00243, w0: 0.005, w1: 0.088, b:-0.794, W.grad: tensor([-6.7169e-05, -5.8781e-05]), b.grad:tensor([0.0017])\n",
      "[Epoch: 5,900] loss: 0.00242, w0: 0.005, w1: 0.088, b:-0.796, W.grad: tensor([-6.4721e-05, -5.6290e-05]), b.grad:tensor([0.0017])\n",
      "[Epoch: 6,000] loss: 0.00242, w0: 0.005, w1: 0.088, b:-0.797, W.grad: tensor([-6.2566e-05, -5.4075e-05]), b.grad:tensor([0.0016])\n",
      "[Epoch: 6,100] loss: 0.00242, w0: 0.005, w1: 0.088, b:-0.799, W.grad: tensor([-6.0221e-05, -5.1734e-05]), b.grad:tensor([0.0015])\n",
      "[Epoch: 6,200] loss: 0.00242, w0: 0.005, w1: 0.088, b:-0.800, W.grad: tensor([-5.7982e-05, -4.9517e-05]), b.grad:tensor([0.0015])\n",
      "[Epoch: 6,300] loss: 0.00241, w0: 0.005, w1: 0.088, b:-0.802, W.grad: tensor([-5.6049e-05, -4.7577e-05]), b.grad:tensor([0.0014])\n",
      "[Epoch: 6,400] loss: 0.00241, w0: 0.005, w1: 0.088, b:-0.803, W.grad: tensor([-5.4132e-05, -4.5704e-05]), b.grad:tensor([0.0014])\n",
      "[Epoch: 6,500] loss: 0.00241, w0: 0.005, w1: 0.088, b:-0.805, W.grad: tensor([-5.2039e-05, -4.3677e-05]), b.grad:tensor([0.0013])\n",
      "[Epoch: 6,600] loss: 0.00241, w0: 0.005, w1: 0.088, b:-0.806, W.grad: tensor([-5.0332e-05, -4.1999e-05]), b.grad:tensor([0.0013])\n",
      "[Epoch: 6,700] loss: 0.00241, w0: 0.005, w1: 0.088, b:-0.807, W.grad: tensor([-4.8489e-05, -4.0238e-05]), b.grad:tensor([0.0012])\n",
      "[Epoch: 6,800] loss: 0.00241, w0: 0.006, w1: 0.088, b:-0.808, W.grad: tensor([-4.6529e-05, -3.8390e-05]), b.grad:tensor([0.0012])\n",
      "[Epoch: 6,900] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.810, W.grad: tensor([-4.4980e-05, -3.6924e-05]), b.grad:tensor([0.0011])\n",
      "[Epoch: 7,000] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.811, W.grad: tensor([-4.3343e-05, -3.5398e-05]), b.grad:tensor([0.0011])\n",
      "[Epoch: 7,100] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.812, W.grad: tensor([-4.1701e-05, -3.3874e-05]), b.grad:tensor([0.0011])\n",
      "[Epoch: 7,200] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.813, W.grad: tensor([-4.0348e-05, -3.2635e-05]), b.grad:tensor([0.0010])\n",
      "[Epoch: 7,300] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.814, W.grad: tensor([-3.8913e-05, -3.1311e-05]), b.grad:tensor([0.0010])\n",
      "[Epoch: 7,400] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.815, W.grad: tensor([-3.7457e-05, -3.0010e-05]), b.grad:tensor([0.0009])\n",
      "[Epoch: 7,500] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.816, W.grad: tensor([-3.6048e-05, -2.8737e-05]), b.grad:tensor([0.0009])\n",
      "[Epoch: 7,600] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.817, W.grad: tensor([-3.4913e-05, -2.7724e-05]), b.grad:tensor([0.0009])\n",
      "[Epoch: 7,700] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.817, W.grad: tensor([-3.3601e-05, -2.6565e-05]), b.grad:tensor([0.0008])\n",
      "[Epoch: 7,800] loss: 0.00240, w0: 0.006, w1: 0.088, b:-0.818, W.grad: tensor([-3.2571e-05, -2.5655e-05]), b.grad:tensor([0.0008])\n",
      "[Epoch: 7,900] loss: 0.00239, w0: 0.006, w1: 0.088, b:-0.819, W.grad: tensor([-3.1201e-05, -2.4449e-05]), b.grad:tensor([0.0008])\n",
      "[Epoch: 8,000] loss: 0.00239, w0: 0.006, w1: 0.088, b:-0.820, W.grad: tensor([-3.0153e-05, -2.3546e-05]), b.grad:tensor([0.0007])\n",
      "[Epoch: 8,100] loss: 0.00239, w0: 0.006, w1: 0.088, b:-0.820, W.grad: tensor([-2.9146e-05, -2.2677e-05]), b.grad:tensor([0.0007])\n",
      "[Epoch: 8,200] loss: 0.00239, w0: 0.006, w1: 0.088, b:-0.821, W.grad: tensor([-2.7938e-05, -2.1650e-05]), b.grad:tensor([0.0007])\n",
      "[Epoch: 8,300] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.822, W.grad: tensor([-2.6950e-05, -2.0797e-05]), b.grad:tensor([0.0007])\n",
      "[Epoch: 8,400] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.822, W.grad: tensor([-2.5927e-05, -1.9920e-05]), b.grad:tensor([0.0006])\n",
      "[Epoch: 8,500] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.823, W.grad: tensor([-2.4990e-05, -1.9141e-05]), b.grad:tensor([0.0006])\n",
      "[Epoch: 8,600] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.824, W.grad: tensor([-2.4060e-05, -1.8363e-05]), b.grad:tensor([0.0006])\n",
      "[Epoch: 8,700] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.824, W.grad: tensor([-2.3109e-05, -1.7566e-05]), b.grad:tensor([0.0006])\n",
      "[Epoch: 8,800] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.825, W.grad: tensor([-2.2425e-05, -1.7012e-05]), b.grad:tensor([0.0005])\n",
      "[Epoch: 8,900] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.825, W.grad: tensor([-2.1561e-05, -1.6318e-05]), b.grad:tensor([0.0005])\n",
      "[Epoch: 9,000] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.826, W.grad: tensor([-2.0890e-05, -1.5767e-05]), b.grad:tensor([0.0005])\n",
      "[Epoch: 9,100] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.826, W.grad: tensor([-2.0212e-05, -1.5223e-05]), b.grad:tensor([0.0005])\n",
      "[Epoch: 9,200] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.827, W.grad: tensor([-1.9368e-05, -1.4521e-05]), b.grad:tensor([0.0005])\n",
      "[Epoch: 9,300] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.827, W.grad: tensor([-1.8540e-05, -1.3841e-05]), b.grad:tensor([0.0005])\n",
      "[Epoch: 9,400] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.828, W.grad: tensor([-1.7975e-05, -1.3407e-05]), b.grad:tensor([0.0004])\n",
      "[Epoch: 9,500] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.828, W.grad: tensor([-1.7294e-05, -1.2858e-05]), b.grad:tensor([0.0004])\n",
      "[Epoch: 9,600] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.829, W.grad: tensor([-1.6575e-05, -1.2276e-05]), b.grad:tensor([0.0004])\n",
      "[Epoch: 9,700] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.829, W.grad: tensor([-1.6037e-05, -1.1866e-05]), b.grad:tensor([0.0004])\n",
      "[Epoch: 9,800] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.829, W.grad: tensor([-1.5612e-05, -1.1542e-05]), b.grad:tensor([0.0004])\n",
      "[Epoch: 9,900] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.830, W.grad: tensor([-1.4742e-05, -1.0837e-05]), b.grad:tensor([0.0004])\n",
      "[Epoch:10,000] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.830, W.grad: tensor([-1.4475e-05, -1.0657e-05]), b.grad:tensor([0.0003])\n",
      "[Epoch:10,100] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.830, W.grad: tensor([-1.3826e-05, -1.0138e-05]), b.grad:tensor([0.0003])\n",
      "[Epoch:10,200] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.831, W.grad: tensor([-1.3464e-05, -9.8745e-06]), b.grad:tensor([0.0003])\n",
      "[Epoch:10,300] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.831, W.grad: tensor([-1.2747e-05, -9.2921e-06]), b.grad:tensor([0.0003])\n",
      "[Epoch:10,400] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.831, W.grad: tensor([-1.2573e-05, -9.2027e-06]), b.grad:tensor([0.0003])\n",
      "[Epoch:10,500] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.832, W.grad: tensor([-1.1879e-05, -8.6365e-06]), b.grad:tensor([0.0003])\n",
      "[Epoch:10,600] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.832, W.grad: tensor([-1.1621e-05, -8.4614e-06]), b.grad:tensor([0.0003])\n",
      "[Epoch:10,700] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.832, W.grad: tensor([-1.0895e-05, -7.8759e-06]), b.grad:tensor([0.0003])\n",
      "[Epoch:10,800] loss: 0.00239, w0: 0.006, w1: 0.089, b:-0.832, W.grad: tensor([-1.0757e-05, -7.7977e-06]), b.grad:tensor([0.0003])\n",
      "[Epoch:10,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.833, W.grad: tensor([-1.0200e-05, -7.3711e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,000] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.833, W.grad: tensor([-9.9406e-06, -7.1600e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,100] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.833, W.grad: tensor([-9.6168e-06, -6.9477e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,200] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.833, W.grad: tensor([-9.2378e-06, -6.6708e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,300] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.834, W.grad: tensor([-8.8240e-06, -6.3280e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,400] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.834, W.grad: tensor([-8.5837e-06, -6.1765e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,500] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.834, W.grad: tensor([-8.1696e-06, -5.8611e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,600] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.834, W.grad: tensor([-7.8923e-06, -5.6339e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,700] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.834, W.grad: tensor([-7.7120e-06, -5.5271e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,800] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.835, W.grad: tensor([-7.2233e-06, -5.1508e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:11,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.835, W.grad: tensor([-6.9126e-06, -4.9130e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:12,000] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.835, W.grad: tensor([-6.7943e-06, -4.8236e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:12,100] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.835, W.grad: tensor([-6.5478e-06, -4.6678e-06]), b.grad:tensor([0.0002])\n",
      "[Epoch:12,200] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.835, W.grad: tensor([-6.2641e-06, -4.4666e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:12,300] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.835, W.grad: tensor([-5.9139e-06, -4.1854e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:12,400] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.836, W.grad: tensor([-5.8313e-06, -4.1270e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:12,500] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.836, W.grad: tensor([-5.6780e-06, -4.0320e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:12,600] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.836, W.grad: tensor([-5.6556e-06, -4.0630e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:12,700] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.836, W.grad: tensor([-5.2949e-06, -3.7812e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:12,800] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.836, W.grad: tensor([-4.8981e-06, -3.4509e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:12,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.836, W.grad: tensor([-4.8438e-06, -3.4142e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:13,000] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.836, W.grad: tensor([-4.6998e-06, -3.3050e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:13,100] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.836, W.grad: tensor([-4.6665e-06, -3.3192e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:13,200] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.836, W.grad: tensor([-4.2866e-06, -3.0374e-06]), b.grad:tensor([0.0001])\n",
      "[Epoch:13,300] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-4.3282e-06, -3.1050e-06]), b.grad:tensor([9.8316e-05])\n",
      "[Epoch:13,400] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-3.9600e-06, -2.7778e-06]), b.grad:tensor([9.4712e-05])\n",
      "[Epoch:13,500] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-3.7638e-06, -2.6102e-06]), b.grad:tensor([9.1175e-05])\n",
      "[Epoch:13,600] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-3.7849e-06, -2.6400e-06]), b.grad:tensor([8.7775e-05])\n",
      "[Epoch:13,700] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-3.6523e-06, -2.5711e-06]), b.grad:tensor([8.4514e-05])\n",
      "[Epoch:13,800] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-3.5844e-06, -2.5456e-06]), b.grad:tensor([8.1345e-05])\n",
      "[Epoch:13,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-3.3813e-06, -2.4065e-06]), b.grad:tensor([7.8344e-05])\n",
      "[Epoch:14,000] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-3.3795e-06, -2.4444e-06]), b.grad:tensor([7.5398e-05])\n",
      "[Epoch:14,100] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-2.8685e-06, -2.0042e-06]), b.grad:tensor([7.2640e-05])\n",
      "[Epoch:14,200] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-2.9563e-06, -2.0837e-06]), b.grad:tensor([6.9914e-05])\n",
      "[Epoch:14,300] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-2.7207e-06, -1.8670e-06]), b.grad:tensor([6.7323e-05])\n",
      "[Epoch:14,400] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.837, W.grad: tensor([-2.7282e-06, -1.8726e-06]), b.grad:tensor([6.4832e-05])\n",
      "[Epoch:14,500] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-2.6136e-06, -1.7819e-06]), b.grad:tensor([6.2367e-05])\n",
      "[Epoch:14,600] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-2.5456e-06, -1.7645e-06]), b.grad:tensor([6.0102e-05])\n",
      "[Epoch:14,700] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-2.5673e-06, -1.8198e-06]), b.grad:tensor([5.7833e-05])\n",
      "[Epoch:14,800] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-2.3013e-06, -1.6329e-06]), b.grad:tensor([5.5691e-05])\n",
      "[Epoch:14,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-2.2886e-06, -1.6391e-06]), b.grad:tensor([5.3649e-05])\n",
      "[Epoch:15,000] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-2.3985e-06, -1.7732e-06]), b.grad:tensor([5.1601e-05])\n",
      "[Epoch:15,100] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.9325e-06, -1.3889e-06]), b.grad:tensor([4.9716e-05])\n",
      "[Epoch:15,200] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.9421e-06, -1.3846e-06]), b.grad:tensor([4.7904e-05])\n",
      "[Epoch:15,300] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.8794e-06, -1.3361e-06]), b.grad:tensor([4.6095e-05])\n",
      "[Epoch:15,400] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.7822e-06, -1.2325e-06]), b.grad:tensor([4.4342e-05])\n",
      "[Epoch:15,500] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.6727e-06, -1.1306e-06]), b.grad:tensor([4.2762e-05])\n",
      "[Epoch:15,600] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.7829e-06, -1.2163e-06]), b.grad:tensor([4.1171e-05])\n",
      "[Epoch:15,700] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.7975e-06, -1.2287e-06]), b.grad:tensor([3.9586e-05])\n",
      "[Epoch:15,800] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.6813e-06, -1.1437e-06]), b.grad:tensor([3.8113e-05])\n",
      "[Epoch:15,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.5671e-06, -1.0766e-06]), b.grad:tensor([3.6757e-05])\n",
      "[Epoch:16,000] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.5389e-06, -1.0673e-06]), b.grad:tensor([3.5399e-05])\n",
      "[Epoch:16,100] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.5413e-06, -1.0822e-06]), b.grad:tensor([3.4041e-05])\n",
      "[Epoch:16,200] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.4069e-06, -1.0027e-06]), b.grad:tensor([3.2705e-05])\n",
      "[Epoch:16,300] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.4137e-06, -1.0294e-06]), b.grad:tensor([3.1572e-05])\n",
      "[Epoch:16,400] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.4057e-06, -1.0667e-06]), b.grad:tensor([3.0441e-05])\n",
      "[Epoch:16,500] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.3908e-06, -1.0853e-06]), b.grad:tensor([2.9307e-05])\n",
      "[Epoch:16,600] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.4187e-06, -1.1238e-06]), b.grad:tensor([2.8175e-05])\n",
      "[Epoch:16,700] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-1.4075e-06, -1.1362e-06]), b.grad:tensor([2.7040e-05])\n",
      "[Epoch:16,800] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.838, W.grad: tensor([-9.6671e-07, -7.5872e-07]), b.grad:tensor([2.6122e-05])\n",
      "[Epoch:16,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-9.2387e-07, -7.2271e-07]), b.grad:tensor([2.5215e-05])\n",
      "[Epoch:17,000] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-1.0282e-06, -8.1460e-07]), b.grad:tensor([2.4302e-05])\n",
      "[Epoch:17,100] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-9.7913e-07, -7.6741e-07]), b.grad:tensor([2.3398e-05])\n",
      "[Epoch:17,200] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-9.3753e-07, -7.3140e-07]), b.grad:tensor([2.2495e-05])\n",
      "[Epoch:17,300] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-1.0657e-06, -8.4192e-07]), b.grad:tensor([2.1581e-05])\n",
      "[Epoch:17,400] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-6.3082e-07, -4.5324e-07]), b.grad:tensor([2.0752e-05])\n",
      "[Epoch:17,500] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-6.9352e-07, -4.7560e-07]), b.grad:tensor([2.0068e-05])\n",
      "[Epoch:17,600] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-6.1436e-07, -3.8433e-07]), b.grad:tensor([1.9396e-05])\n",
      "[Epoch:17,700] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-7.0284e-07, -4.4331e-07]), b.grad:tensor([1.8709e-05])\n",
      "[Epoch:17,800] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-6.4385e-07, -3.8433e-07]), b.grad:tensor([1.8035e-05])\n",
      "[Epoch:17,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-7.0035e-07, -4.3027e-07]), b.grad:tensor([1.7352e-05])\n",
      "[Epoch:18,000] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-7.1836e-07, -4.3524e-07]), b.grad:tensor([1.6673e-05])\n",
      "[Epoch:18,100] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-7.0656e-07, -4.1661e-07]), b.grad:tensor([1.5993e-05])\n",
      "[Epoch:18,200] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-7.1463e-07, -4.0233e-07]), b.grad:tensor([1.5314e-05])\n",
      "[Epoch:18,300] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-6.1467e-07, -3.1789e-07]), b.grad:tensor([1.4727e-05])\n",
      "[Epoch:18,400] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-5.7463e-07, -3.1851e-07]), b.grad:tensor([1.4277e-05])\n",
      "[Epoch:18,500] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-6.2026e-07, -3.7812e-07]), b.grad:tensor([1.3820e-05])\n",
      "[Epoch:18,600] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-5.8146e-07, -3.6694e-07]), b.grad:tensor([1.3372e-05])\n",
      "[Epoch:18,700] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-5.1316e-07, -3.1106e-07]), b.grad:tensor([1.2921e-05])\n",
      "[Epoch:18,800] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-4.3710e-07, -2.6946e-07]), b.grad:tensor([1.2472e-05])\n",
      "[Epoch:18,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-5.5693e-07, -3.8991e-07]), b.grad:tensor([1.2009e-05])\n",
      "[Epoch:19,000] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-4.3865e-07, -2.9430e-07]), b.grad:tensor([1.1565e-05])\n",
      "[Epoch:19,100] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-4.9671e-07, -3.5080e-07]), b.grad:tensor([1.1109e-05])\n",
      "[Epoch:19,200] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-5.8704e-07, -4.3213e-07]), b.grad:tensor([1.0651e-05])\n",
      "[Epoch:19,300] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-4.9981e-07, -3.6135e-07]), b.grad:tensor([1.0204e-05])\n",
      "[Epoch:19,400] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-4.5945e-07, -3.2658e-07]), b.grad:tensor([9.7537e-06])\n",
      "[Epoch:19,500] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-4.9174e-07, -3.6384e-07]), b.grad:tensor([9.2996e-06])\n",
      "[Epoch:19,600] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-3.8122e-07, -2.7567e-07]), b.grad:tensor([8.8963e-06])\n",
      "[Epoch:19,700] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-3.0920e-07, -2.4090e-07]), b.grad:tensor([8.6756e-06])\n",
      "[Epoch:19,800] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-3.8991e-07, -3.3031e-07]), b.grad:tensor([8.4400e-06])\n",
      "[Epoch:19,900] loss: 0.00239, w0: 0.007, w1: 0.089, b:-0.839, W.grad: tensor([-3.5887e-07, -3.3900e-07]), b.grad:tensor([8.2161e-06])\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "  W = torch.ones((2,))\n",
    "  b = torch.zeros((1,))\n",
    "\n",
    "  simple_dataset = SimpleDataset()\n",
    "  train_data_loader = DataLoader(dataset=simple_dataset, batch_size=len(simple_dataset))\n",
    "  batch = next(iter(train_data_loader))\n",
    "\n",
    "  input, target = batch\n",
    "\n",
    "  y_pred = model(input, W, b)\n",
    "  print(y_pred.shape)\n",
    "  print(y_pred)\n",
    "\n",
    "  loss = loss_fn(y_pred, target)\n",
    "  print(loss)\n",
    "\n",
    "  learn(W, b, train_data_loader)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "link_dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
