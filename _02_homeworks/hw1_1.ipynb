{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a_tensor_initialization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor class\n",
    "t1 = torch.Tensor([1, 2, 3], device='cpu')\n",
    "print(t1.dtype)   # >>> torch.float32\n",
    "print(t1.device)  # >>> cpu\n",
    "print(t1.requires_grad)  # >>> False\n",
    "print(t1.size())  # torch.Size([3])\n",
    "print(t1.shape)   # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t1_cuda = t1.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t1_cuda = t1.cuda()\n",
    "\n",
    "t1_cpu = t1.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tensor : Multi-dimensional array containing elements of a **single data type**\n",
    "* device : data type of tensor. By default, torch.Tensor() creates a tensor of type **torch.float32**\n",
    "* requires_grad : Gradient Calculation, checks whether the tensor is set up for gradient calculation\n",
    "* t1.size() / t1.shape : both return the size (or dimensions) of the tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### torch.shape / torch.size()와 rank의 개념\n",
    "\n",
    "* torch.shape / torch.size() : 텐서 차원의 크기.\n",
    "* rank : 텐서가 몇 차원인지, 즉 몇 개의 **축**이 있는지.  \n",
    "<br>\n",
    "\n",
    "* tensor에서 차원이 늘어날수록 shape의 ***왼쪽으로*** 추가됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "# torch.tensor function\n",
    "t2 = torch.tensor([1, 2, 3], device='cpu')\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "# if you have gpu device\n",
    "# t2_cuda = t2.to(torch.device('cuda'))\n",
    "# or you can use shorthand\n",
    "# t2_cuda = t2.cuda()\n",
    "t2_cpu = t2.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch.tensor : **automatically infers** the data type from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "torch.Size([1]) 1\n",
      "torch.Size([5]) 1\n",
      "torch.Size([5, 1]) 2\n",
      "torch.Size([3, 2]) 2\n",
      "torch.Size([3, 2, 1]) 3\n",
      "torch.Size([3, 1, 2, 1]) 4\n",
      "torch.Size([3, 1, 2, 3]) 4\n",
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "torch.Size([4, 5]) 2\n",
      "torch.Size([4, 1, 5]) 3\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(1)\t\t\t     # shape: torch.Size([]), ndims(=rank): 0\n",
    "print(a1.shape, a1.ndim)\n",
    "\n",
    "a2 = torch.tensor([1])\t\t  \t     # shape: torch.Size([1]), ndims(=rank): 1\n",
    "print(a2.shape, a2.ndim)\n",
    "\n",
    "a3 = torch.tensor([1, 2, 3, 4, 5])   # shape: torch.Size([5]), ndims(=rank): 1\n",
    "print(a3.shape, a3.ndim)\n",
    "\n",
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])   # shape: torch.Size([5, 1]), ndims(=rank): 2\n",
    "print(a4.shape, a4.ndim)\n",
    "\n",
    "a5 = torch.tensor([                 # shape: torch.Size([3, 2]), ndims(=rank): 2\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim)\n",
    "\n",
    "a6 = torch.tensor([                 # shape: torch.Size([3, 2, 1]), ndims(=rank): 3\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim)\n",
    "\n",
    "a7 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 1]), ndims(=rank): 4\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim)\n",
    "\n",
    "a8 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3]), ndims(=rank): 4\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim)\n",
    "\n",
    "\n",
    "a9 = torch.tensor([                 # shape: torch.Size([3, 1, 2, 3, 1]), ndims(=rank): 5\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 5]), ndims(=rank): 2\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim)\n",
    "\n",
    "a10 = torch.tensor([                 # shape: torch.Size([4, 1, 5]), ndims(=rank): 3\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 괄호의 개수로 tensor의 차원을, 각 괄호안의 원소 개수로 shape을 결정한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "#     [[[1, 2, 3], [4, 5]]],\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Each dimension must have the same size\n",
    "\n",
    "* shape으로 표현해보면 (4,1,2,**3 or 2**) 가 된다. 즉 `,` 로 나누어진 원소들의 개수는 같아야 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b_tensor_initialization_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch.Tensor() converts a list to a tensor by **copying** the data into a new memory space.<br>So, modifying the list l1 does not affect the tensor t1.  \n",
    "\n",
    "* torch.as_tensor() : tensor and the list reference the **same memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch.as_tensor() shares the **same memory** as the original NumPy array. <br>This means that changes to the NumPy array will directly affect the tensor.  \n",
    "* Modifying `l6[0] = 100` updates both the array and the tensor since **they reference the same memory.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c_tensor_initialization_constant_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "tensor([1., 0., 0., 0.])\n",
      "tensor([1., 0., 0., 0.])\n",
      "\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(size=(5,))  # or torch.ones(5)\n",
    "t1_like = torch.ones_like(input=t1)\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print()\n",
    "\n",
    "t2 = torch.zeros(size=(6,))  # or torch.zeros(6)\n",
    "t2_like = torch.zeros_like(input=t2)\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print()\n",
    "\n",
    "t3 = torch.empty(size=(4,))  # or torch.zeros(4)\n",
    "t3_like = torch.empty_like(input=t3)\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "print()\n",
    "\n",
    "t4 = torch.eye(n=3)\n",
    "print(t4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* torch.ones/zeros : Creates a tensor of size (n,) with all elements initialized to `1.0 / 0.0`\n",
    "* torch.empty : Creates a tensor of size (n,), but the values **are uninitialized**\n",
    "* torch.eye = Creates a 3x3 **identity matrix** that has 1.0 on the diagonal and 0.0 elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## d_tensor_initialization_random_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 14]])\n",
      "tensor([[0.7983, 0.5869, 0.4204]])\n",
      "tensor([[ 1.1059,  0.6547, -1.1423]])\n",
      "tensor([[ 9.4662,  9.6387],\n",
      "        [ 9.3230, 10.1985],\n",
      "        [12.0873, 12.3524]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.randint(low=10, high=20, size=(1, 2))\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3))\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.randn(size=(1, 3))\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2))\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3)\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5)\n",
    "print(t6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "  \n",
    "* `torch.randint()` : generates random integers between a specified range\n",
    "* torch.rand : generate float values from a **uniform distribution between 0 and 1**\n",
    "* torch.randn : generates float values from a **normal distribution with a mean of 0 and a standard deviation of 1**\n",
    "* torch.normal() : generates random numbers from a **normal distribution with a specified mean and standard**\n",
    "* torch.linspace() : generates n(step) evenly spaced values between a start and end point.\n",
    "* torch.arange() : generates values in a range with a given step. <br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code demonstrates the use of torch.manual_seed() to set a seed for the random number generator in PyTorch.\n",
    "By setting the seed, the random number generator produces the same sequence of random numbers\n",
    "* `random1` is the first random tensor generated with the **seed 1729**, and `random2` is the **next random tensor** generated in sequence.   \n",
    "random1 and random2 will have different values.\n",
    "* random3 will have the same values as random1, as both are generated after setting the seed to 1729.  \n",
    "Similarly, random4 will match random2 because they are both generated in the same sequence after resetting the seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## e_tensor_type_conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2, 3))\n",
    "print(a.dtype)\n",
    "print()\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16)\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20.\n",
    "print(c)\n",
    "print()\n",
    "\n",
    "d = b.to(torch.int32)\n",
    "print(d)\n",
    "print()\n",
    "\n",
    "double_d = torch.ones(10, 2, dtype=torch.double)\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).double()\n",
    "short_e = torch.ones(10, 2).short()\n",
    "\n",
    "double_d = torch.zeros(10, 2).to(torch.double)\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short)\n",
    "\n",
    "double_d = torch.zeros(10, 2).type(torch.double)\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short)\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double)\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### This code demonstrates various ways to set and convert the **data types** (dtype) of tensors in PyTorch.\n",
    "* `b.to(torch.int32)` converts the int16 tensor b to int32.  \n",
    "* `.type(torch.double)` or `.type(dtype=torch.short)`: Another way to change the data type using the type() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f_tensor_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "############################## 1\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "############################## 2\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "############################## 3\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.ones(size=(2, 3))\n",
    "t2 = torch.ones(size=(2, 3))\n",
    "t3 = torch.add(t1, t2)\n",
    "t4 = t1 + t2\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "print(\"#\" * 30, 1)\n",
    "\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30, 2)\n",
    "\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30, 3)\n",
    "\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "* This code demonstrates basic arithmetic operations between tensors in PyTorch using both function calls like   \n",
    "`torch.add()`, `torch.sub()`, `torch.mul()`, `torch.div()`, and operator syntax (`+`, `-`, `*`, `/`).  \n",
    "Both methods produce the same result, and you can use either depending on your preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이 때의 연산은 **Element-wise Operation (요소별 연산)**으로써 두 텐서가 같은 크기(Shape)를 가질 때, 각 위치의 원소끼리 연산을 수행하는 방식이다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## g_tensor_operations_mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1])\n",
    ")\n",
    "print(t1, t1.size())\n",
    "print() \n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3)\n",
    "print(t4, t4.size())\n",
    "print()\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6)\n",
    "print(t7.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `torch.dot()`: This function calculates the dot product of two 1D tensors (vectors) of the same size.  \n",
    "Since t1 is a **scalar**, its size is **torch.Size([])**<br><br>\n",
    "* `torch.mm()`: This function performs matrix multiplication between 2D matrices.  \n",
    "The matrix multiplication is valid when the number of columns in t2 matches the number of rows in t3.  \n",
    "**m x n  by n x p : m x p**<br><br>\n",
    "* `torch.bmm()`: This function performs batch matrix multiplication. It allows you to multiply multiple pairs of matrices at once.<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `torch.mm`은 행렬의 곱셉만 지원하기에, vetor의 내적은 불가능하다  \n",
    "또한 broadcasting은 지원하지 않으므로 Debug에서 유리하다.  \n",
    "추가적으로, batch 차원까지 연산을 지원해주지는 않는다. 2차원 행렬끼리의 곱만 연산가능하다.<br><br>\n",
    "* `torch.bmm`은 3차원 텐서의 곱 연산만 지원한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h_tensor_operations_matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `torch.matmul()` supports different types of tensor multiplication.  \n",
    "* Vector x vector multiplication results in a **scalar** (dot product).  \n",
    "* Matrix x vector multiplication produces a **vector**, and batched matrix multiplication produces **tensors** where each matrix in the batch is multiplied.  \n",
    "* Batched matrix x matrix multiplication multiplies corresponding matrices in each batch,  \n",
    "and matrix x single matrix multiplication multiplies each matrix in the batch with a single matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## i_tensor_broadcasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n",
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `t1 * t2` multiplies each element of tensor t1 by the scalar t2 -> **element wise**  \n",
    "* `t3 - t4` applies broadcasting, where t4 is expanded to match the size of t3, and **element-wise** subtraction is performed.  \n",
    "* t6 is a 3D tensor of size (3, 28, 28), representing an **image** with 3 channels (e.g., RGB).  \n",
    "`The normalize(x)` function divides each element of the tensor x by 255 to scale the pixel values to the range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n"
     ]
    }
   ],
   "source": [
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the dimension sizes of the two tensors, going from last to first  \n",
    "* t12: A 2D tensor of size (3, 2). The missing first dimension is expanded to match t11's size 4  \n",
    "* t14: A 2D tensor of size (3, 1). The third dimension (size 1) is broadcasted to match t13's third dimension (size 2)  \n",
    "* t16: A 2D tensor of size (1, 2). The second dimension is broadcasted to match t15's second dimension  \n",
    "* t18: A 3D tensor of size (3, 1, 1). Its third and fourth dimensions (size 1) are broadcasted to match t17  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Broadcasting allows **smaller tensors** to be expanded to match the dimensions of **larger tensors** for operations.  \n",
    "When dimensions don’t align, an error occurs.  \n",
    "* torch.pow() computes the element-wise power for each tensor element, allowing operations with scalars or between two tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## j_tensor_indexing_slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(\n",
    "  [[0, 1, 2, 3, 4],\n",
    "   [5, 6, 7, 8, 9],\n",
    "   [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9])\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11])\n",
    "print(x[1, 2])  # >>> tensor(7)\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `x[1]` : the second row of the 2D tensor x. \n",
    "* `x[:, 1]` : the second column from all rows. \n",
    "* `x[1, 2]` : the third element from the second row. \n",
    "* `x[:, -1]` : the last column from all rows. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.zeros((6, 6))\n",
    "y[1:4, 2] = 1\n",
    "print(y)\n",
    "print()\n",
    "print(y[1:4, 1:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `y[1:4, 2] = 1`: Sets the values of the second column in rows 1 to 3 to 1.  \n",
    "* `y[1:4, 1:4]` : Selects the sub-tensor containing rows 1 to 3 and columns 1 to 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor(\n",
    "  [[1, 2, 3, 4],\n",
    "   [2, 3, 4, 5],\n",
    "   [5, 6, 7, 8]]\n",
    ")\n",
    "print(z[:2])\n",
    "print()\n",
    "print(z[1:, 1:3])\n",
    "print()\n",
    "print(z[:, 1:])\n",
    "print()\n",
    "\n",
    "z[1:, 1:3] = 0\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `z[:2]` : Selects the first two rows. The result is [[1, 2, 3, 4], [2, 3, 4, 5]].\n",
    "* `z[1:, 1:3]` : Selects the sub-tensor starting from the second row and second to third columns. The result is [[3, 4], [6, 7]].\n",
    "* `z[:, 1:]` : Selects all rows and the second to last columns. The result is [[2, 3, 4], [3, 4, 5], [6, 7, 8]].\n",
    "* `z[1:, 1:3] = 0`: Sets the values of the sub-tensor starting from the second row and second to third columns to 0. The modified tensor z becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k_tensor_reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "tensor([[1, 2, 3, 4, 5, 6]])\n",
      "\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2)\n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print(t2)\n",
    "print()\n",
    "print(t3)\n",
    "print()\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4)\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3)\n",
    "print(t4)\n",
    "print()\n",
    "print(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* code demonstrates how to change the shape of a tensor using view() and reshape()  \n",
    "Both functions are used to transform the shape of a tensor, but they **can differ** in their internal operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "\n",
      "tensor([1, 4, 2, 5, 3, 6])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(t_transposed\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mt_transposed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
     ]
    }
   ],
   "source": [
    "# Additional Code(for difference view() and reshape())\n",
    "\n",
    "t = torch.tensor([[1,2,3],\n",
    "                  [4,5,6]]) # 2x3 tensor\n",
    "\n",
    "t_transposed = t.t() # 3x2 tensor\n",
    "print(t_transposed)\n",
    "print()\n",
    "print(t_transposed.reshape(6))\n",
    "print(t_transposed.view(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor의 메모리 연속성\n",
    "\n",
    "* PyTorch에서 **tensor는 메모리에 연속적으로 저장된다**. 위의 2차원 텐서 t는 2x3 size의 텐서이며  \n",
    "메모리에 연속적으로 저장된다 [1, 2, 3, 4, 5, 6]  \n",
    "이 값들은 행 우선(row-major) 방식으로 저장된다 ([1, 2, 3] -> [4, 5, 6])  \n",
    "* tensor를 전치하면, [[1, 4], [2, 5], [3, 6]]인 3x2 텐서로 바뀌게 되는데\n",
    "중요한 것은 전치 연산을 하더라도, PyTorch는 데이터를 복사하지 않고 **기존 메모리 배치를 그대로 유지한다.**   \n",
    "즉 메모리 상의 데이터는 여전히 [1, 2, 3, 4, 5, 6] 순서로 저장되어 있다.  \n",
    "\n",
    "* transposed tensor에서, 1st row [1, 4]는 원래 메모리 상에서 떨어진, stride가 1이 아닌 값들이므로  \n",
    "이 tensor는 **non-contiguous가 된다.**\n",
    "\n",
    "* `view()`는 tensor의 data를 **copy하지 않고,** 그저 데이터를 어떻게 해석할 것인지를 바꾸는 함수이므로 메모리상에 있는 데이터가 연속적이어야 한다.  \n",
    "전치된 텐서의 첫 번째 행 [1, 4]는 실제 메모리 상에서 연속된 상태가 아니므로 view()가 제대로 작동하지 않는다.  \n",
    "\n",
    "* `reshape()`의 경우, 메모리 상의 연속성 여부에 상관없이 텐서의 모양을 변경할 수 있다.  \n",
    "만약 데이터가 연속적이라면 view()처럼 동작하고, 연속적이지 않다면 **데이터를 복사하여** 새로운 연속적인 메모리 블록에 데이터를 배치한 후, 그 데이터를 원하는 모양으로 바꾼다.  \n",
    "즉, transposed tensor의 경우 reshape()은 이를 감지하고, 데이터를 새로운 연속적인 메모리 공간에 복사하여 [1, 4, 2, 5, 3, 6] 순서로 배열한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]])\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,)\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1)\n",
    "print(t7)\n",
    "print(t8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tensor has the following structure:\n",
    "\n",
    "* First dimension: size 1  \n",
    "* Second dimension: size 3\n",
    "* Third dimension: size 1<br><br>\n",
    "* `t6.squeeze()` removes **all dimensions** that have size 1. Since t6 has a shape of (1, 3, 1), the first and third dimensions are removed.  \n",
    "* `t6.squeeze(0)` removes the first dimension (index 0). Since the first dimension of t6 has a size of 1, it can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1)\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3)\n",
    "print(t12, t12.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `t9.unsqueeze(1)` : This adds a new dimension at position 1. So, the tensor becomes a 2D tensor with shape (3, 1)  \n",
    "Each element gets a new axis of size 1 along the second dimension.<br><br>\n",
    "* `unsqueeze()` 함수는 parameter없이 사용할 수 없다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,)\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]])\n",
    "t16 = torch.flatten(t15)\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1)\n",
    "\n",
    "print(t16)\n",
    "print(t17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `t13 = torch.tensor([[1, 2, 3], [4, 5, 6]])` : t13 is a 2D tensor with shape (2, 3).  \n",
    "* `t13.flatten()` : The flatten() function flattens the tensor into a 1D array.<br><br>  \n",
    "* `torch.flatten(t15, start_dim=1)` : This flatten() allows you to specify the dimension from which to start flattening using the **start_dim parameter.**  \n",
    "in this case, `start_dim=1` flattens all dimensions **after the first dimension.** It flattens each 2D sub-tensor along dimension 1 into a 1D tensor, resulting in a tensor with shape (2, 4).<br><br>\n",
    "\n",
    "* 즉, 첫 번째 차원 이후에 있는 차원들을 모두 하나의 차원으로 압축하는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5])\n",
      "torch.Size([5, 2, 3])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "t18 = torch.randn(2, 3, 5)\n",
    "print(t18.shape)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)).size())  # >>> torch.Size([5, 2, 3])\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print(t20)\n",
    "print(t21)\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t22)\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2)\n",
    "\n",
    "print(t23)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `torch.permute(t18, (2, 0, 1))` : The permute() function rearranges the dimensions of the tensor. The tuple (2, 0, 1) specifies the new order of the dimensions.  \n",
    "this case, the original shape (2, 3, 5) is rearranged, the first dimension -> the third, the second dimension -> the first, and the third dimension -> the second.   \n",
    "This results in a shape of (5, 2, 3).<br><br>\n",
    "* `torch.permute(t19, dims=(0, 1)) / torch.permute(t19, dims=(1, 0))` : dims=(0, 1) is left unchanged, so the shape remains (2, 3).  \n",
    "dims=(1, 0) case, the dimensions are swapped, The result is a shape of (3, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## l_tensor_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1)\n",
    "print(t4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `cat()` function concatenates the tensors along the specified dimension.  \n",
    "this case, the tensors are concatenated along the second dimension `(dim=1)`  <br><br> \n",
    "* Concatenating along `dim=1` means that the values along the second dimension will be joined together.  \n",
    "The 1st and 3rd dimensions remain the same, but the second dimension will be the sum of the second dimensions of t1, t2, and t3,  \n",
    "resulting in a new tensor of shape (2, 6, 3) (since 1 + 3 + 2 = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0)\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* t5 creates a 1D tensor with values [0, 1, 2], with shape(3,)  \n",
    "* t6 creates a 1D tensor with values [3, 4, 5, 6, 7], with shape(5, )  \n",
    "* The cat() function concatenates the tensors. Here, `dim=0` means concatenating along the first dimension, resulting shape(8, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0)\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1)\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code demonstrates how to use PyTorch's torch.cat() function to concatenate 2D tensors along different dimensions  \n",
    "* `dim=0` means concatenating along the first dimension (rows), resulting shape(2+2, 3)\n",
    "* `dim=1` means concatenating along the second dimension (columns), resulting shpae(2, 3+3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n"
     ]
    }
   ],
   "source": [
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0)\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1)\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above code demonstrates how to use PyTorch's `torch.cat()` function to concatenate 2D tensors along different dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0)\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1)\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2)\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above code demonstrates how to use PyTorch's `torch.cat()` function to concatenate 3D tensors along different dimensions(dim = 0,1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m_tensor_stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "print(t3.shape, t3.equal(t4))\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1)\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "print(t5.shape, t5.equal(t6))\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2)\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2)\n",
    "print(t7.shape, t7.equal(t8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `torch.stack((t9, t10), dim=0)` : `stack()` adds a new dimension along which the tensors are stacked<BR><BR>\n",
    "* `t9.unsqueeze(0)` : `unsqueeze(0)` adds a new dimension at the first axis, converting the 1D tensor into a 2D tensor.  <BR><BR>\n",
    "* `torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)` : Then, cat() concatenates the tensors along the first axis (dim=0). The result is the same as using stack()  <BR><BR>\n",
    "* `stack()`은 주어진 텐서들의 모양을 유지하면서, 새로운 차원을 추가하여 텐서들을 쌓는다. 또한, 모든 텐서가 동일한 크기여야 한다. <BR>즉, 각 텐서의 모양이 일치해야만 stack()이 작동한다.  dim parameter는 새로운 차원이 추가될 위치를 결정한다고 생각하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n_tensor_vstack_hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.tensor([1, 2, 3])\n",
    "t2 = torch.tensor([4, 5, 6])\n",
    "t3 = torch.vstack((t1, t2))\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]])\n",
    "t5 = torch.tensor([[4], [5], [6]])\n",
    "t6 = torch.vstack((t4, t5))\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8])\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above code demonstrates how to use `torch.vstack()` function to vertically stack multiple tensors along the first dimension (dim=0).  \n",
    "* `torch.vstack((t1, t2))` : vstack() stacks the two 1D tensors vertically, along the first dimension (dim=0), converting them into a 2D tensor.  \n",
    "The result is a (2, 3) 2D tensor.  <BR><BR>\n",
    "* t7 : 3D tensor(shape : (2, 2, 3))  \n",
    "t8 : 3D tensor(shape : (2, 2, 3))  \n",
    "`torch.vstack([t7, t8])` : Stacks the two 3D tensors along the first dimension (dim=0), resulting in a tensor of shape (4, 2, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    }
   ],
   "source": [
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Above code demonstrates how to use `torch.hstack()` function to horizontally stack multiple tensors along the second dimension (dim=1).  \n",
    "* `torch.hstack((t10, t11))` : hstack() concatenates the two 1D tensors horizontally, resulting in a 1D tensor with a size of 6.\n",
    "The result is a (2, 3) 2D tensor.  <BR><BR>\n",
    "* t13: A 2D tensor of shape (3, 1)  \n",
    "t14: Another 2D tensor of shape (3, 1)  \n",
    "`torch.hstack((t13, t14))` : hstack() concatenates the two 2D tensors horizontally along the second dimension (dim=1), resulting in a tensor with a shape of (3, 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vstack은 dim=0을 기준으로 텐서를 수직으로 쌓고, hstack은 dim=1을 기준으로 텐서를 수평하게 쌓는다.  \n",
    "* dim=2를 쌓으려면 별도의 함수가 아닌 `torch.cat((t1, t2), dim=2) / torch.stack((t1, t2), dim=2)` 와 같이 쌓을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 숙제 후기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 과제는 마치 수학을 처음 배우는 순간으로 돌아간 것 같다. 그러나 이번에는 코드를 통해 확인해보고 돌려보는 방식이다.   \n",
    "이 과제를 한마디로 요약하자면, 딥러닝을 배우기 전에 **텐서**에 대한 모든 것을 배우는 시간이라고 할 수 있다.  \n",
    "인공지능이나 머신러닝, 혹은 선형대수를 공부한 경험이 있는 사람이라면 이 개념들이 익숙하게 느껴질 수 있다.   \n",
    "그러나 코드를 통해 텐서의 개념을 구현하면서 수학적인 원리와 코딩 구현 사이에 **미묘하지만 중요한 차이**가 있다는 것을 깨닫게 되었다.\n",
    "\n",
    "텐서는 현대 AI 시스템, 특히 PyTorch나 TensorFlow와 같은 딥러닝 프레임워크에서 모든 연산의 근간을 이루는데,  \n",
    "딥러닝은 기본적으로 행렬과 벡터 연산에 기반을 두고 있으며, 고차원 데이터는 텐서 조작을 통해 처리된다.   \n",
    "따라서 이러한 연산을 깊이 있게 이해하는 것은 향후 더 복잡한 개념을 배울 때 매우 중요한 기초가 된다.\n",
    "\n",
    "과제를 진행하면서 브로드캐스팅, 행렬 곱셈, 텐서 변환, 그리고 다양한 요소별 연산을 직접 경험할 수 있었다.   \n",
    "수학적으로는 간단한 연산처럼 보일 수 있지만, PyTorch에서 이러한 연산을 구현할 때는 텐서의 크기와 차원을 세심하게 관리하는 것이 중요하다는 것을 깨달았다.    \n",
    "\n",
    "이번 과제에서 특히 도전적이었던 부분은 **Broadcasting**이었다. 브로드캐스팅은 서로 다른 크기의 텐서 간에 연산을 가능하게 해주는 편리한 개념이지만,   \n",
    "실제로 언제 어떻게 브로드캐스팅이 적용되는지를 이해하는 것은 매우 중요하다.    \n",
    "이 과제를 통해 브로드캐스팅이 연산을 얼마나 단순화하는 동시에 계산 효율성을 유지할 수 있는지에 대해 깊이 배울 수 있었고,  \n",
    "텐서가 자동으로 특정 차원에 맞춰 확장되거나 정렬되는 방식을 이해하는 것은 소중한 경험이었다.   실제 데이터는 종종 다양한 모양과 크기로 들어오기 때문에,   \n",
    "이를 처리하는 능력이 중요할 것 같았다.\n",
    "\n",
    "또한 행렬 곱셈도 간단하지만 중요한 개념이라고 생각한다.    \n",
    "딥러닝에서 신경망의 가중치 행렬이 입력 벡터와 곱해져 다음 층의 활성화를 만드는 원리를 배우는 것은 필수적인데, 수학적으로는 행렬 곱셈이 잘 알려져 있지만,  \n",
    "이를 **효율적으로 코드로 구현하는 방법**, 특히 PyTorch의 torch.matmul()과 같은 함수의 사용법을 익히고, GPU에서 병렬 처리를 통해 최적화되는 방식에 대해 배울 수 있었다.\n",
    "\n",
    "과제를 진행하면서 딥러닝의 본질이 결국 수학에 있다는 사실을 다시 한번 실감했다.    \n",
    "텐서는 기본적으로 벡터와 행렬의 일반화된 형태이므로, 선형대수에 대한 **탄탄한 기초가 텐서를 이해하는 데 직접적으로 연결된다는 것**을 깨달았지만   \n",
    "수학적 개념을 넘어서, PyTorch에서 텐서를 다루는 것은 또 다른 의미로 중요하다고 생각한다.   \n",
    "특히 텐서의 차원을 관리하고, 연산의 호환성을 유지하며, 성능을 최적화하는 것이 꼭 해봐야 할 경험이었다고 생각한다.\n",
    "\n",
    "결론적으로, 이번 과제는 PyTorch 사용법을 배우는 것 이상의 의미를 가지고 있었다.   \n",
    "이는 딥러닝의 수학적 기초를 더욱 깊이 이해할 수 있는 기회였고, 다시 한번 나의 지식을 확인할 수 있는 시간이었다.  \n",
    "이번 과제를 통해 텐서에 대한 심도 있는 경험을 쌓았으며, 앞으로 AI와 머신러닝에서 더 복잡한 주제로 나아갈 수 있는 탄탄한 기초를 다질 수 있었다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
